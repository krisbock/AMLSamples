{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Churn Prediction using Azure Machine Learning service\n",
    "\n",
    "This notebook will introduce the use of the churn dataset to create churn prediction model using Stochastic variational deep kernel learning technique\n",
    "\n",
    "The dataset used to ingest is from SIDKDD 2009 competition. \n",
    "\n",
    "The pipeline is composed using Azure ML pipeline and trained on Azure ML compute with hyper parameters of the gaussian process and the neural network jointly tunned through hyperdrive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK verison 1.0.23\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "from azureml.core import  (Workspace,Run,VERSION,\n",
    "                           Experiment,Datastore)\n",
    "from azureml.core.runconfig import (RunConfiguration,\n",
    "                                    DEFAULT_GPU_IMAGE)\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.compute import (AmlCompute, ComputeTarget)\n",
    "from azureml.exceptions import ComputeTargetException\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import (Pipeline, \n",
    "                                   PipelineData)\n",
    "from azureml.pipeline.steps import (HyperDriveStep,PythonScriptStep)\n",
    "from azureml.train.dnn import PyTorch\n",
    "from azureml.train.hyperdrive import *\n",
    "from azureml.widgets import RunDetails\n",
    "\n",
    "\n",
    "print('SDK verison', VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables declaration\n",
    "\n",
    "Declare variables to be used through out, please fill in the Azure subscription ID, resource-group and workspace name to connect to your Azure ML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBSCRIPTION_ID = 'fe375bc2-9f1a-4909-ad0d-9319806d5e97'\n",
    "RESOURCE_GROUP = 'amlenv_rg'\n",
    "WORKSPACE_NAME = 'vienna'\n",
    "\n",
    "PROJECT_DIR = os.getcwd()\n",
    "EXPERIMENT_NAME = \"customer_churn\"\n",
    "CLUSTER_NAME = \"gpu-cluster\"\n",
    "DATA_DIR = os.path.join(PROJECT_DIR,'data')\n",
    "SCRIPT_DIR = os.path.join(PROJECT_DIR,'train')\n",
    "\n",
    "SOURCE_URL ='https://amlgitsamples.blob.core.windows.net/churn'\n",
    "FILE_NAME = 'CATelcoCustomerChurnTrainingSample.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize workspace\n",
    "\n",
    "Initialize a workspace object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace loaded: vienna\n"
     ]
    }
   ],
   "source": [
    "ws = Workspace(workspace_name = WORKSPACE_NAME,\n",
    "               subscription_id = SUBSCRIPTION_ID ,\n",
    "               resource_group = RESOURCE_GROUP)\n",
    "\n",
    "ws.write_config()\n",
    "\n",
    "print('Workspace loaded:', ws.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data download\n",
    "\n",
    "Download Dataset locally to experiment folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/extdrive1/home/sasuke/dev/amlsamples/Customer_churn/data/CATelcoCustomerChurnTrainingSample.csv',\n",
       " <http.client.HTTPMessage at 0x7f77e1ca8828>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "\n",
    "urllib.request.urlretrieve(os.path.join(SOURCE_URL,FILE_NAME), \n",
    "                           filename = os.path.join(DATA_DIR,FILE_NAME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload  dataset to blob datastore\n",
    "\n",
    "Upload dataset to workspace default blob storage which will be mounted on AzureML compute during pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading /extdrive1/home/sasuke/dev/amlsamples/Customer_churn/data/CATelcoCustomerChurnTrainingSample.csv\n",
      "Uploaded /extdrive1/home/sasuke/dev/amlsamples/Customer_churn/data/CATelcoCustomerChurnTrainingSample.csv, 1 files out of an estimated total of 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "$AZUREML_DATAREFERENCE_1154f9288ce2442eab3e569057dbd51a"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_store = default_datastore=ws.datastores[\"workspaceblobstore\"]\n",
    "default_store.upload(src_dir=DATA_DIR, target_path='churn', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve or create a Azure Machine Learning compute\n",
    "\n",
    "Let's create a new Azure Machine Learning Compute in the current workspace, if it doesn't already exist. We will then run the training script on this compute target.\n",
    "\n",
    "If you have already created an Azure ML compute in your workspace, just provide it's name in the cell below to have it used for Azure ML pipeline execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu-cluster found\n"
     ]
    }
   ],
   "source": [
    "cluster_name = \"gpu-cluster\"\n",
    "\n",
    "try:\n",
    "    cluster = ComputeTarget(ws, cluster_name)\n",
    "    print(cluster_name, \"found\")\n",
    "    \n",
    "except ComputeTargetException:\n",
    "    print(cluster_name, \"not found, provisioning....\")\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_NC6',max_nodes=1)\n",
    "\n",
    "    \n",
    "    cluster = ComputeTarget.create(ws, cluster_name, provisioning_config)\n",
    "    cluster.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline definition\n",
    "\n",
    "\n",
    "The Azure ML pipeline is composed of two steps: \n",
    " \n",
    " - Data pre-processing which consist of one-hot encoding categorical features, normalization of the features set, spliting of dataset into training/testing sets and finally writing out the output to storage.\n",
    " \n",
    " - Hyperdrive step that tune and train the deep kernel learning model using GPytorch and Pytorch estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline data input/output\n",
    "\n",
    "Here, we define the input and intermediary dataset that will be used by the pipeline steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = DataReference(datastore=default_store,\n",
    "                          data_reference_name=\"input_data\",\n",
    "                          path_on_datastore=\"churn\"\n",
    "                         )\n",
    "\n",
    "processed_dir = PipelineData(name = 'processed_data',\n",
    "                             datastore=default_store\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1st step: Data Preprocessing\n",
    "\n",
    "We start by defining the run configuration with the needed dependencies by the preprocessing step.\n",
    "\n",
    "In the cell that follow, we compose the first step of the pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = CondaDependencies()\n",
    "cd.add_conda_package('pandas')\n",
    "cd.add_conda_package('matplotlib')\n",
    "cd.add_conda_package('numpy')\n",
    "cd.add_conda_package('scikit-learn')\n",
    "\n",
    "\n",
    "run_config = RunConfiguration(framework=\"python\",\n",
    "                              conda_dependencies= cd)\n",
    "run_config.target = cluster\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = DEFAULT_GPU_IMAGE\n",
    "run_config.environment.python.user_managed_dependencies = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processing = PythonScriptStep(\n",
    "                            name = 'preprocess dataset',\n",
    "                            script_name = 'preprocess.py',\n",
    "                            arguments = ['--input_path', input_dir,\\\n",
    "                                         '--output_path', processed_dir],\n",
    "                            inputs = [input_dir],\n",
    "                            outputs = [processed_dir],\n",
    "                            compute_target = cluster_name,\n",
    "                            runconfig = run_config\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline second step: training\n",
    "\n",
    "For the second step, we start by defining the pytorch estimator that will be used to traing the Stochastic variational deep kernel learning model using Gpytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(source_directory=SCRIPT_DIR,\n",
    "                    conda_packages = ['pandas', 'numpy', 'scikit-learn'],\n",
    "                    pip_packages = ['gpytorch'],\n",
    "                    compute_target=cluster,\n",
    "                    entry_script='svdkl_entry.py',\n",
    "                    use_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we configure Hyperdrive by defining the hyperparametes space. Note given the unbalanced distribution of the independent variable, we choose Area under the curve as the metric to optimize for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = RandomParameterSampling(\n",
    "    {\n",
    "        '--batch-size': choice(4096),\n",
    "        '--epochs': choice(500),\n",
    "        '--neural-net-lr': loguniform(-4,-2),\n",
    "        '--likelihood-lr': loguniform(-4,-2),\n",
    "        '--grid-size': choice(32,64),\n",
    "        '--grid-bounds': choice(-1,0),\n",
    "        '--latent-dim': choice(2),\n",
    "        '--num-mixtures': choice(4,6,8)\n",
    "    }\n",
    ")\n",
    "\n",
    "early_termination_policy = BanditPolicy(evaluation_interval=10, slack_factor=0.1)\n",
    "\n",
    "hd_config = HyperDriveRunConfig(estimator=estimator, \n",
    "                                hyperparameter_sampling=ps,\n",
    "                                policy=early_termination_policy,\n",
    "                                primary_metric_name='auc', \n",
    "                                primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n",
    "                                max_total_runs=20,\n",
    "                                max_concurrent_runs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, we define the hyperdrive step of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hd_step = HyperDriveStep(\n",
    "    name=\"hyper parameters tunning\",\n",
    "    hyperdrive_run_config=hd_config,\n",
    "    estimator_entry_script_arguments=['--data-folder', processed_dir],\n",
    "    inputs=[processed_dir])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build & Execute pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created step hyper parameters tunning [ba763580][8bd50569-f7d8-4fe6-a5f9-341ada7b5c04], (This step will run and generate new outputs)\n",
      "Created step preprocess dataset [82998361][07bd784a-f319-423b-a943-6789e383d91e], (This step will run and generate new outputs)\n",
      "Created data reference input_data for StepId [478ff87e][0c5d950c-515c-4e42-b881-335f9aab7361], (Consumers of this data will generate new runs.)\n",
      "Submitted pipeline run: 5836e1f9-38e5-494e-8385-3dc41a092b47\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[hd_step],\n",
    "                    default_datastore=default_store\n",
    "                   )\n",
    "pipeline_run = Experiment(ws, 'Customer_churn').submit(pipeline,\n",
    "                                                      regenerate_outputs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "993e9a49ecf94511b1a42acba6bb04bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_PipelineWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(pipeline_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_run.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:amlenv]",
   "language": "python",
   "name": "conda-env-amlenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
